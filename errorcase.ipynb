{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "import os, time\n",
    "import math\n",
    "\n",
    "import argparse\n",
    "import tabulate\n",
    "\n",
    "import utils, models, ml_algorithm\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset, Subset, TensorDataset\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.distributions import Normal\n",
    "from collections import defaultdict\n",
    "from prettytable import PrettyTable\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## MinMax Scaling Functions ------------------------------------\n",
    "def minmax_col(data, name):\n",
    "    minval , maxval = data[name].min(), data[name].max()\n",
    "    data[name]=(data[name]-data[name].min())/(data[name].max()-data[name].min())\n",
    "    return minval, maxval\n",
    "\n",
    "def minmax_tensor(tensor):\n",
    "    minvals = tensor.min()\n",
    "    maxvals = tensor.max()\n",
    "    \n",
    "    normalized = (tensor - minvals) / (maxvals - minvals)\n",
    "    return normalized, minvals, maxvals\n",
    "\n",
    "def restore_minmax(data, minv, maxv):\n",
    "    minv=0 if minv==None else minv\n",
    "    maxv=0 if maxv==None else maxv\n",
    "    data = (data * (maxv - minv)) + minv\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tabledata(Dataset):\n",
    "    def __init__(self, model_name, data, scale='minmax', binary_t=False):\n",
    "        self.use_treatment = not (model_name == 'iTransformer')\n",
    "        # padding tensors\n",
    "        self.diff_tensor = torch.zeros([124,1])\n",
    "        if self.use_treatment:\n",
    "            if not model_name == 'cevae':\n",
    "                self.cont_tensor = torch.zeros([124,3])\n",
    "            else:\n",
    "                self.cont_tensor = torch.zeros([124,4])\n",
    "        else:\n",
    "            self.cont_tensor = torch.zeros([124,5])\n",
    "\n",
    "        self.cat_tensor = torch.zeros([124,7])\n",
    "        yd=[]\n",
    "        for _, group in data.groupby('cluster'):\n",
    "            yd.append(group[['y', 'd']].tail(1))\n",
    "        yd = pd.concat(yd)\n",
    "\n",
    "        ## 데이터 전처리 ##\n",
    "        # 연속 데이터 정규화 #\n",
    "        for c in [\"age\", \"dis\", \"danger\", \"CT_R\", \"CT_E\"]:\n",
    "            # dis : 0~6\n",
    "            # danger : 3~11\n",
    "            minmax_col(data, c) \n",
    "        self.a_y, self.b_y = minmax_col(yd,\"y\")\n",
    "        self.a_d, self.b_d = minmax_col(yd,\"d\")\n",
    "\n",
    "        ## 데이터 특성 별 분류 및 저장 ##\n",
    "        self.cluster = data.iloc[:,0].values.astype('float32')\n",
    "        \n",
    "        if not binary_t:\n",
    "            self.treatment = data[['dis', 'danger']].values.astype('float32') if not model_name == 'cevae' else data['danger'].values.astype('float32')\n",
    "        else:\n",
    "            raise('do not use binary t')\n",
    "            print(\"use binary t\")\n",
    "            self.treatment = (data['dis'].values >= 0.5).astype('float32')\n",
    "            \n",
    "        if self.use_treatment:\n",
    "            drop_col = ['dis'] if model_name == 'cevae' else ['dis', 'danger']\n",
    "            self.cont_X = data.iloc[:, 1:6].drop(columns=drop_col).values.astype('float32')\n",
    "        else:\n",
    "            self.cont_X = data.iloc[:, 1:6].values.astype('float32')\n",
    "        \n",
    "        self.cat_X = data.iloc[:, 6:13].astype('category')\n",
    "        self.diff_days = data.iloc[:, 13].values.astype('float32')\n",
    "\n",
    "        # y label tukey transformation\n",
    "        # self.y = yd.values.astype('float32')\n",
    "        y = torch.tensor(yd['y'].values.astype('float32'))\n",
    "        d = torch.tensor(yd['d'].values.astype('float32'))\n",
    "        \n",
    "        self.yd = torch.stack([y, d], dim=1)\n",
    "        \n",
    "        # 이산 데이터 정렬 및 저장#\n",
    "        self.cat_cols = self.cat_X.columns\n",
    "        self.cat_map = {col: {cat: i for i, cat in enumerate(self.cat_X[col].cat.categories)} for col in self.cat_cols}\n",
    "        self.cat_X = self.cat_X.apply(lambda x: x.cat.codes)\n",
    "        self.cat_X = torch.from_numpy(self.cat_X.to_numpy()).long()\n",
    "    def __len__(self):\n",
    "        return len(np.unique(self.cluster))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "            [batch x padding x embedding]\n",
    "            cont_tensor_p : 패딩이 씌워진 환자 관련 연속 데이터  \n",
    "            cont_tensor_c : 패딩이 씌워진 클러스터 관련 연속 데이터  \n",
    "            cat_tensor_p : 패딩이 씌워진 환자 관련 이산 데이터  \n",
    "            cat_tensor_c : 패딩이 씌워진 클러스터 관련 이산 데이터  \n",
    "            data_len : 클러스터별 유효 환자수 반환 데이터\n",
    "            y : 정답 label\n",
    "            diff_tensor : 클러스터별 유효 날짜 반환 데이터\n",
    "        '''\n",
    "        diff_days = torch.from_numpy(self.diff_days[self.cluster == index]).unsqueeze(1)\n",
    "        diff_tensor = self.diff_tensor.clone()\n",
    "        diff_tensor[:diff_days.shape[0]] = diff_days\n",
    "        cont_X = torch.from_numpy(self.cont_X[self.cluster == index])\n",
    "        data_len = cont_X.shape[0]\n",
    "        cont_tensor = self.cont_tensor.clone()\n",
    "        cont_tensor[:cont_X.shape[0],] = cont_X\n",
    "        cat_X = self.cat_X[self.cluster == index]\n",
    "        cat_tensor = self.cat_tensor.clone()\n",
    "        cat_tensor[:cat_X.shape[0],] = cat_X\n",
    "        cat_tensor_p = cat_tensor[:, :5]\n",
    "        cat_tensor_c = cat_tensor[:, 5:]\n",
    "        cont_tensor_p = cont_tensor[:, :3]\n",
    "        cont_tensor_c = cont_tensor[:, 3:]\n",
    "        yd = self.yd[index]\n",
    "        \n",
    "        treatment = torch.mean(torch.tensor(self.treatment[self.cluster == index]), dim=0) # t1: dis|t2: danger\n",
    "        return cont_tensor_p, cont_tensor_c, cat_tensor_p, cat_tensor_c, data_len, yd, diff_tensor, treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training Clusters : 1270\n",
      "Successfully load data!\n"
     ]
    }
   ],
   "source": [
    "dataset = Tabledata('cevae', pd.read_csv('/data1/bubble3jh/cluster-regression/data/'+f\"data_cut_0.csv\"), 'minmax')\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, utils.data_split_num(dataset))\n",
    "tr_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "print(f\"Number of training Clusters : {len(train_dataset)}\")\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "print(\"Successfully load data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEVAEEmbedding(torch.nn.Module):\n",
    "    '''\n",
    "        output_size : embedding output의 크기\n",
    "        disable_embedding : 연속 데이터의 임베딩 유무\n",
    "        disable_pe : transformer의 sequance 기준 positional encoding add 유무\n",
    "        reduction : \"mean\" : cluster 별 평균으로 reduction\n",
    "                    \"date\" : cluster 내 date 평균으로 reduction\n",
    "    '''\n",
    "    def __init__(self,treatments, output_size=128, disable_embedding=False, disable_pe=True, reduction=\"date\", shift=False, use_treatment = False):\n",
    "        super().__init__()\n",
    "        self.shift = shift\n",
    "        self.reduction = reduction\n",
    "        self.disable_embedding = disable_embedding\n",
    "        self.disable_pe = disable_pe\n",
    "        activation = nn.ELU()\n",
    "        if not disable_embedding:\n",
    "            print(\"Embedding applied to data\")\n",
    "            nn_dim = emb_hidden_dim = emb_dim = output_size//4\n",
    "            if treatments=='single':\n",
    "                self.cont_c_NN = nn.Sequential(nn.Linear(1 if use_treatment else 2, emb_hidden_dim),\n",
    "                                    activation,\n",
    "                                    nn.Linear(emb_hidden_dim, nn_dim))\n",
    "            else:\n",
    "                nn_dim = nn_dim * 2\n",
    "                self.cont_c_NN = None\n",
    "            self.cont_p_NN = nn.Sequential(nn.Linear(3 , emb_hidden_dim),\n",
    "                                        activation,\n",
    "                                        nn.Linear(emb_hidden_dim, nn_dim))\n",
    "        else:\n",
    "            emb_dim_p = 5\n",
    "            emb_dim_c = 2\n",
    "        self.lookup_gender  = nn.Embedding(2, emb_dim)\n",
    "        self.lookup_korean  = nn.Embedding(2, emb_dim)\n",
    "        self.lookup_primary  = nn.Embedding(2, emb_dim)\n",
    "        self.lookup_job  = nn.Embedding(11, emb_dim)\n",
    "        self.lookup_rep  = nn.Embedding(34, emb_dim)\n",
    "        self.lookup_place  = nn.Embedding(19, emb_dim)\n",
    "        self.lookup_add  = nn.Embedding(31, emb_dim)\n",
    "        if not disable_pe:\n",
    "            if shift:\n",
    "                self.positional_embedding  = nn.Embedding(6, output_size)\n",
    "            else:\n",
    "                self.positional_embedding  = nn.Embedding(5, output_size)\n",
    "            # self.positional_embedding = SinusoidalPositionalEncoding(output_size)\n",
    "\n",
    "    def forward(self, cont_p, cont_c, cat_p, cat_c, val_len, diff_days):\n",
    "        if not self.disable_embedding:\n",
    "            cont_p_emb = self.cont_p_NN(cont_p)\n",
    "            cont_c_emb = self.cont_c_NN(cont_c) if self.cont_c_NN != None else None\n",
    "                \n",
    "        a1_embs = self.lookup_gender(cat_p[:,:,0].to(torch.int))\n",
    "        a2_embs = self.lookup_korean(cat_p[:,:,1].to(torch.int))\n",
    "        a3_embs = self.lookup_primary(cat_p[:,:,2].to(torch.int))\n",
    "        a4_embs = self.lookup_job(cat_p[:,:,3].to(torch.int))\n",
    "        a5_embs = self.lookup_rep(cat_p[:,:,4].to(torch.int))\n",
    "        a6_embs = self.lookup_place(cat_c[:,:,0].to(torch.int))\n",
    "        a7_embs = self.lookup_add(cat_c[:,:,1].to(torch.int))\n",
    "        \n",
    "        cat_p_emb = torch.mean(torch.stack([a1_embs, a2_embs, a3_embs, a4_embs, a5_embs]), axis=0)\n",
    "        cat_c_emb = torch.mean(torch.stack([a6_embs, a7_embs]), axis=0)\n",
    "\n",
    "        if not self.disable_embedding:\n",
    "            tensors_to_concat = [tensor for tensor in [cat_p_emb, cat_c_emb, cont_p_emb, cont_c_emb] if tensor is not None]\n",
    "            x = torch.cat(tensors_to_concat, dim=2)\n",
    "            # x = torch.cat((cat_p_emb, cat_c_emb, cont_p_emb, cont_c_emb), dim=2)\n",
    "        else:\n",
    "            x = torch.cat((cat_p_emb, cat_c_emb, cont_p, cont_c), dim=2)\n",
    "            \n",
    "        if not self.disable_pe:\n",
    "            x = x + self.positional_embedding(diff_days.int().squeeze(2))\n",
    "        # return reduction_cluster(x, diff_days, val_len, self.reduction)\n",
    "        if self.reduction == \"none\":   \n",
    "            if self.shift:\n",
    "                return (x, diff_days, val_len), self.positional_embedding(torch.tensor([5]).cuda())\n",
    "            else:\n",
    "                return (x, diff_days, val_len), None\n",
    "        else:\n",
    "            return models.reduction_cluster(x, diff_days, val_len, self.reduction)\n",
    "        \n",
    "\n",
    "class CETransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CETransformer, self).__init__()\n",
    "        d_model=64\n",
    "        nhead=4\n",
    "        d_hid=128\n",
    "        nlayers=5\n",
    "        dropout=0.1\n",
    "        pred_layers=1\n",
    "        self.shift = False\n",
    "        self.unidir = False\n",
    "        self.is_variational = False\n",
    "        self.is_synthetic = False\n",
    "        \n",
    "        if self.is_variational:\n",
    "            print(\"variational z sampling\")\n",
    "        else:\n",
    "            print(\"determinant z \")\n",
    "            \n",
    "        if self.unidir:\n",
    "            print(\"unidirectional attention applied\")\n",
    "        else:\n",
    "            print(\"maxpool applied\")\n",
    "        self.embedding = CEVAEEmbedding(treatments='double',output_size=d_model, disable_embedding = False, disable_pe=False, reduction=\"none\", shift= False, use_treatment=True)\n",
    "         \n",
    "        encoder_layers = models.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_encoder = models.customTransformerEncoder(encoder_layers, nlayers, d_model, pred_layers=pred_layers, residual_t=False, residual_x=True)\n",
    "\n",
    "        # Vairatioanl Z\n",
    "        self.fc_mu = nn.Linear(d_model, d_model)\n",
    "        self.fc_logvar = nn.Linear(d_model, d_model)\n",
    "\n",
    "        decoder_layers = models.TransformerDecoderLayer(d_model, nhead, d_hid, dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer_decoder = models.TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=124, stride=1)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.z2t = models.MLP(d_model, d_model//2, 1, num_layers=pred_layers)\n",
    "        self.t1_emb = models.MLP(1, d_model//2, d_model, num_layers=pred_layers)\n",
    "        self.t2_emb = models.MLP(1, d_model//2, d_model, num_layers=pred_layers)\n",
    "        self.zt12t2 = models.MLP(d_model, d_model//2, 1, num_layers=pred_layers)\n",
    "        self.zt2yd = models.MLP(d_model, d_model//2, 2, num_layers=pred_layers)\n",
    "\n",
    "        self.linear_decoder = models.MLP(d_model, d_model, d_model, num_layers=1) # Linear\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.masked_fill(mask == 0, True).masked_fill(mask == 1, False)\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self, c):\n",
    "        initrange = 0.1\n",
    "        # For embedding layers\n",
    "        if hasattr(self.embedding, 'weight'):\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        # For transformer encoder and decoder\n",
    "        for module in [self.transformer_encoder, self.transformer_decoder]:\n",
    "            for param in module.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "        \n",
    "        # For models.MLP layers\n",
    "        for mlp in [self.z2t, self.t_emb, self.zt2yd]:\n",
    "            for layer in mlp.layers:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    layer.weight.data.uniform_(-initrange, initrange)\n",
    "                    if layer.bias is not None:\n",
    "                        layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, cont_p, cont_c, cat_p, cat_c, val_len, diff_days, is_MAP=False):\n",
    "        # Encoder\n",
    "        if self.embedding.reduction != \"none\":\n",
    "            x = self.embedding(cont_p, cont_c, cat_p, cat_c, val_len, diff_days).unsqueeze(1)\n",
    "        else:\n",
    "            (x, diff_days, _), start_tok = self.embedding(cont_p, cont_c, cat_p, cat_c, val_len, diff_days) # embedded:(32, 124, 128)\n",
    "        index_tensor = torch.arange(x.size(1), device=x.device)[None, :, None]\n",
    "        src_key_padding_mask = ~(torch.arange(x.size(1)).expand(x.size(0), -1).cuda() < val_len.unsqueeze(1)).cuda()\n",
    "        src_mask = self.generate_square_subsequent_mask(x.size(1)).cuda() if self.unidir else None\n",
    "        \n",
    "        # Z ------\n",
    "        # CETransformer encoder\n",
    "        z, (enc_t1, enc_t2), enc_yd = self.transformer_encoder(x, mask=src_mask, src_key_padding_mask=src_key_padding_mask, val_len=val_len)\n",
    "        if self.unidir:\n",
    "            idx = val_len - 1\n",
    "            z = z[torch.arange(z.size(0)), idx] # padding 이 아닌값에 해당하는 seq중 마지막 값 사용\n",
    "        else:\n",
    "            val_mask = torch.arange(z.size(1))[None, :].cuda() < val_len[:, None]\n",
    "            valid_z = z * val_mask[:, :, None].float().cuda()\n",
    "            z = valid_z.max(dim=1)[0] # padding 이 아닌값에 해당하는 seq 들 max pool\n",
    "        \n",
    "        # z_mu, z_logvar = self.fc_mu(z), self.fc_logvar(z)\n",
    "        z_mu, z_logvar = z, self.fc_logvar(z)\n",
    "            \n",
    "        if is_MAP:\n",
    "            z=z_mu\n",
    "        elif self.is_variational:\n",
    "            z = models.reparametrize(z_mu, z_logvar)\n",
    "        else:\n",
    "            z_logvar = torch.full_like(z_mu, -100.0).cuda()\n",
    "            z = models.reparametrize(z_mu, z_logvar)\n",
    "        \n",
    "        dec_t1 = self.z2t(z.squeeze())\n",
    "        t1_emb = self.t1_emb(dec_t1)\n",
    "        dec_t2 = self.zt12t2(z.squeeze()+t1_emb)\n",
    "        t2_emb = self.t2_emb(dec_t2)\n",
    "        \n",
    "        # Linear Decoder\n",
    "        dec_yd = self.zt2yd(z.squeeze() + t1_emb + t2_emb)\n",
    "        \n",
    "        pos_embeddings = self.embedding.positional_embedding(diff_days.squeeze().long()) if not self.is_synthetic else torch.zeros_like(z.unsqueeze(1))\n",
    "        \n",
    "        z_expanded = z.unsqueeze(1) + pos_embeddings  # [batch_size, 124, hidden_dim]\n",
    "        z_expanded = torch.where(index_tensor < val_len[:, None, None], z_expanded, torch.zeros_like(z_expanded))\n",
    "        \n",
    "        z_flat = z_expanded.view(-1, z.shape[-1])  # [batch_size * 5, hidden_dim]\n",
    "        x_recon_flat = self.linear_decoder(z_flat)  # [batch_size * 5, hidden_dim]\n",
    "\n",
    "        x_recon = x_recon_flat.view(z_expanded.shape)  # [batch_size, 5, hidden_dim]\n",
    "        \n",
    "        x = torch.where(index_tensor < val_len[:, None, None], x, torch.zeros_like(x))\n",
    "        x_recon = torch.where(index_tensor < val_len[:, None, None], x_recon, torch.zeros_like(x_recon))\n",
    "\n",
    "        return x, x_recon, (enc_yd, torch.cat([enc_t1, enc_t2], dim=1)), (dec_yd, torch.cat([dec_t1, dec_t2], dim=1)), (z_mu, z_logvar)\n",
    "\n",
    "class CEVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CEVAE, self).__init__()\n",
    "        d_model=128\n",
    "        d_hid=128\n",
    "        nlayers=4\n",
    "        dropout=0.1\n",
    "        pred_layers=3\n",
    "        self.shift = False\n",
    "        self.unidir = False\n",
    "        self.is_variational = False\n",
    "        \n",
    "        self.embedding = CEVAEEmbedding(treatments='single', output_size=d_model, disable_embedding = False, disable_pe=True, reduction=\"mean\", shift= self.shift, use_treatment=True)\n",
    "        \n",
    "        self.encoder = models.CEVAE_Encoder(input_dim=d_model, latent_dim=d_hid, hidden_dim=d_model, shared_layers=nlayers, t_pred_layers=pred_layers , pred_layers=pred_layers, drop_out=dropout, t_embed_dim=d_hid, yd_embed_dim=d_hid)\n",
    "        self.decoder = models.CEVAE_Decoder(latent_dim=d_hid, output_dim=d_model, hidden_dim=d_hid, t_pred_layers=pred_layers, shared_layers=nlayers, drop_out=dropout, t_embed_dim=d_hid)\n",
    "\n",
    "    def forward(self, cont_p, cont_c, cat_p, cat_c, _len, diff, t_gt=None, is_MAP=False):\n",
    "        x = self.embedding(cont_p, cont_c, cat_p, cat_c, _len, diff)\n",
    "        z_mu, z_logvar, enc_yd_pred, enc_t_pred = self.encoder(x, t_gt)\n",
    "        \n",
    "        # Sample z using reparametrization trick\n",
    "        if is_MAP:\n",
    "            z=z_mu\n",
    "        elif self.is_variational:\n",
    "            z = models.reparametrize(z_mu, z_logvar)\n",
    "        else:\n",
    "            z_logvar = torch.full_like(z_mu, -100.0).cuda()\n",
    "            z = models.reparametrize(z_mu, z_logvar)\n",
    "        \n",
    "        # Decode z to get the reconstruction of x\n",
    "        dec_t_pred, dec_yd_pred, x_reconstructed = self.decoder(z, t_gt)\n",
    "\n",
    "        return x, x_reconstructed, (enc_yd_pred, torch.stack([enc_t_pred, torch.zeros_like(enc_t_pred)], dim=1)), (dec_yd_pred, torch.stack([dec_t_pred, torch.zeros_like(dec_t_pred)], dim=1)), (z_mu, z_logvar)\n",
    "    \n",
    "    \n",
    "class iTransformer(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=128, output_size=2, num_layers=3, num_heads=8, drop_out=0.0):\n",
    "        super(iTransformer, self).__init__()\n",
    "        self.is_synthetic = False\n",
    "        self.max_len = 124 # hard-coding (seq_len)\n",
    "        \n",
    "        self.embedding = models.TableEmbedding_iTrans(output_size=input_size, disable_pe=True, use_treatment=True)\n",
    "        \n",
    "        # Encoder-only architecture\n",
    "        self.encoder = models.Encoder_iTrans(\n",
    "            [\n",
    "                models.EncoderLayer_iTrans(\n",
    "                    models.AttentionLayer_iTrans(\n",
    "                        models.FullAttention(False, attention_dropout=drop_out), hidden_size, num_heads),\n",
    "                    hidden_size,\n",
    "                    dropout=drop_out,\n",
    "                ) for l in range(num_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(hidden_size)\n",
    "        )\n",
    "        self.projector = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, cont_p, cont_c, cat_p, cat_c, val_len, diff_days):\n",
    "        # Embedding\n",
    "        # B L N -> B N E                (B L N -> B L E in the vanilla Transformer)\n",
    "        (embedded, diff_days, _), _ = self.embedding(cont_p, cont_c, cat_p, cat_c, val_len, diff_days)  # (B, L, E) == (B, N, E)\n",
    "\n",
    "        B, L, E = embedded.shape\n",
    "        N = L\n",
    "        # B: batch_size;    E: d_model; \n",
    "        # L: seq_len;       S: pred_len;\n",
    "        # N: == L\n",
    "        \n",
    "        # B N E -> B N E                (B L E -> B L E in the vanilla Transformer)\n",
    "        # the dimensions of embedded time series has been inverted, and then processed by native attn, layernorm and ffn modules\n",
    "        enc_out = self.encoder(embedded, attn_mask=None)\n",
    "    \n",
    "        # B N E -> B N S -> B S N \n",
    "        dec_out = self.projector(enc_out).permute(0, 2, 1)[:, :, :N] # filter the covariates # (B, 2, L) == (B, 2, N)\n",
    "        return dec_out[:,:,-1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model_name, data, model, scaling, a_y, b_y, a_d, b_d, use_treatment=False, MC_sample=1):\n",
    "    \n",
    "    criterion_mae = nn.L1Loss(reduction=\"sum\")\n",
    "    criterion_rmse = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    batch_num, cont_p, cont_c, cat_p, cat_c, len, y, diff_days, *rest = utils.data_load(data)\n",
    "    out = model(cont_p, cont_c, cat_p, cat_c, len, diff_days)\n",
    "    max_unique_tensor = torch.tensor([batch.unique().max() for batch in diff_days], device='cuda:0') + 1\n",
    "\n",
    "    accumulated_outputs = [0] * 6  # (x, x_reconstructed, enc_yd_pred, enc_t_pred, dec_yd_pred, dec_t_pred)\n",
    "    \n",
    "    if use_treatment:\n",
    "        gt_t = rest[0]\n",
    "        if model_name=='cet' or model_name=='cevae':\n",
    "            for i in range(MC_sample):\n",
    "                out = model(cont_p, cont_c, cat_p, cat_c, len, diff_days)\n",
    "                x, x_reconstructed, (enc_yd_pred, enc_t_pred), (dec_yd_pred, dec_t_pred), (z_mu, z_logvar) = out\n",
    "                \n",
    "                # accumulate predictions\n",
    "                outputs = [x, x_reconstructed, enc_yd_pred, enc_t_pred, dec_yd_pred, dec_t_pred]\n",
    "                accumulated_outputs = [accumulated + output for accumulated, output in zip(accumulated_outputs, outputs)]\n",
    "            \n",
    "            # calculate average\n",
    "            avg_outputs = [accumulated / MC_sample for accumulated in accumulated_outputs]\n",
    "            x, x_reconstructed, enc_yd_pred, enc_t_pred, dec_yd_pred, dec_t_pred = avg_outputs\n",
    "            \n",
    "            \n",
    "            # enc loss\n",
    "            enc_pred_y, enc_pred_d, gt_y, gt_d = utils.reverse_scaling(scaling, enc_yd_pred, y, a_y, b_y, a_d, b_d)\n",
    "            enc_loss_y = criterion_mae(enc_pred_y, gt_y)\n",
    "            enc_loss_d = criterion_mae(enc_pred_d, gt_d)\n",
    "            if not model_name=='cet':\n",
    "                enc_loss_t2 = criterion_mae(enc_t_pred[:,0].squeeze(), gt_t)\n",
    "                enc_loss_t1 = torch.zeros_like(enc_loss_t2)\n",
    "            else:\n",
    "                enc_loss_t1 = criterion_mae(enc_t_pred[:,0].squeeze(), gt_t[:,0])\n",
    "                enc_loss_t2 = criterion_mae(enc_t_pred[:,1].squeeze(), gt_t[:,1])\n",
    "            \n",
    "            # dec loss\n",
    "            dec_pred_y, dec_pred_d, gt_y, gt_d = utils.reverse_scaling(scaling, dec_yd_pred, y, a_y, b_y, a_d, b_d)\n",
    "            dec_loss_y = criterion_mae(dec_pred_y, gt_y)\n",
    "            dec_loss_d = criterion_mae(dec_pred_d, gt_d)\n",
    "            # dec_loss_t = criterion_mae(dec_t_pred.squeeze(), gt_t)\n",
    "            if not model_name=='cet':\n",
    "                dec_loss_t2 = criterion_mae(dec_t_pred[:,0].squeeze(), gt_t)\n",
    "                dec_loss_t1 = torch.zeros_like(dec_loss_t2)\n",
    "            else:\n",
    "                dec_loss_t1 = criterion_mae(dec_t_pred[:,0].squeeze(), gt_t[:,0])\n",
    "                dec_loss_t2 = criterion_mae(dec_t_pred[:,1].squeeze(), gt_t[:,1])\n",
    "\n",
    "            if enc_loss_y + enc_loss_d > dec_loss_y + dec_loss_d:\n",
    "                mae_y, mae_d, loss_t1, loss_t2 = dec_loss_y, dec_loss_d, dec_loss_t1, dec_loss_t2\n",
    "                rmse_y, rmse_d = criterion_rmse(dec_pred_y, gt_y), criterion_rmse(dec_pred_d, gt_d)\n",
    "                out = dec_yd_pred\n",
    "                eval_model = \"Decoder\"\n",
    "            else:\n",
    "                mae_y, mae_d, loss_t1, loss_t2 = enc_loss_y, enc_loss_d, enc_loss_t1, enc_loss_t2\n",
    "                rmse_y, rmse_d = criterion_rmse(enc_pred_y, gt_y), criterion_rmse(enc_pred_d, gt_d)\n",
    "                out = enc_yd_pred\n",
    "                eval_model = \"Encoder\"\n",
    "            mae = mae_y + mae_d\n",
    "            rmse = rmse_y + rmse_d\n",
    "        elif model_name == 'iTransformer':\n",
    "            yd_pred = model(cont_p, cont_c, cat_p, cat_c, len, diff_days)\n",
    "\n",
    "            pred_y, pred_d, gt_y, gt_d = utils.reverse_scaling(scaling, yd_pred, y, a_y, b_y, a_d, b_d)\n",
    "            \n",
    "            # MAE\n",
    "            mae_y = criterion_mae(pred_y, gt_y)\n",
    "            mae_d = criterion_mae(pred_d, gt_d)\n",
    "            mae = mae_y + mae_d\n",
    "            \n",
    "            # RMSE\n",
    "            rmse_y = criterion_rmse(pred_y, gt_y)\n",
    "            rmse_d = criterion_rmse(pred_d, gt_d)\n",
    "            rmse = rmse_y + rmse_d\n",
    "            \n",
    "            if not torch.isnan(mae) and not torch.isnan(rmse):\n",
    "                return mae_d.item(), mae_y.item(), rmse_d.item(), rmse_y.item(), batch_num, yd_pred, y\n",
    "            else:\n",
    "                return 0, batch_num, yd_pred, y\n",
    "    else:\n",
    "        out = model(cont_p, cont_c, cat_p, cat_c, len, diff_days)\n",
    "        if out.shape == torch.Size([2]):\n",
    "            out = out.unsqueeze(0)\n",
    "        pred_y, pred_d, gt_y, gt_d = utils.reverse_scaling(scaling, out, y, a_y, b_y, a_d, b_d)\n",
    "        # MAE\n",
    "        mae_y = criterion_mae(pred_y, gt_y)\n",
    "        mae_d = criterion_mae(pred_d, gt_d)\n",
    "        mae = mae_y + mae_d\n",
    "        \n",
    "        # RMSE\n",
    "        rmse_y = criterion_rmse(pred_y, gt_y)\n",
    "        rmse_d = criterion_rmse(pred_d, gt_d)\n",
    "        rmse = rmse_y + rmse_d\n",
    "        eval_model = \"nan\"\n",
    "    \n",
    "    if not torch.isnan(mae) and not torch.isnan(rmse):\n",
    "        if use_treatment:\n",
    "            return mae_d.item(), mae_y.item(), rmse_d.item(), rmse_y.item(), batch_num, out, y, loss_t1, loss_t2\n",
    "        else:\n",
    "            return mae_d.item(), mae_y.item(), rmse_d.item(), rmse_y.item(), batch_num, out, y\n",
    "    else:\n",
    "        return 0, batch_num, out, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determinant z \n",
      "maxpool applied\n",
      "Embedding applied to data\n",
      "Embedding applied to data\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for iTransformer:\n\tsize mismatch for embedding.cont_c_NN.0.weight: copying a param with shape torch.Size([32, 1]) from checkpoint, the shape in current model is torch.Size([32, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m cet\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_cet-adam-0.01-0.01-0.1-1000-date0_best_val.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m cevae\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_cevae-adam-0.01-0.0001-0.1-1000-date0_best_val.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m \u001b[43mitrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miTransformer-adam-0.001-0.0001-date0_best_val.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 결과 저장을 위한 리스트 초기화\u001b[39;00m\n\u001b[1;32m     18\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/data1/bubble3jh/anaconda3/envs/cet/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for iTransformer:\n\tsize mismatch for embedding.cont_c_NN.0.weight: copying a param with shape torch.Size([32, 1]) from checkpoint, the shape in current model is torch.Size([32, 2])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 모델 초기화\n",
    "cet = CETransformer()\n",
    "cevae = CEVAE()\n",
    "itrans = iTransformer()\n",
    "\n",
    "# 모델 로드\n",
    "model_path = \"/data1/bubble3jh/cluster-regression/best_model_errorcase/seed_1000\"\n",
    "cet.load_state_dict(torch.load(os.path.join(model_path, \"best_cet-adam-0.01-0.01-0.1-1000-date0_best_val.pt\"))['state_dict'])\n",
    "cevae.load_state_dict(torch.load(os.path.join(model_path, \"best_cevae-adam-0.01-0.0001-0.1-1000-date0_best_val.pt\"))['state_dict'])\n",
    "itrans.load_state_dict(torch.load(os.path.join(model_path, \"iTransformer-adam-0.001-0.0001-date0_best_val.pt\"))['state_dict'])\n",
    "\n",
    "# 결과 저장을 위한 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# 테스트 수행\n",
    "for itr, batch_data in enumerate(test_dataloader):\n",
    "    for model in [cet, cevae, iTransformer]:\n",
    "        model_name = model.__class__.__name__\n",
    "        te_mae_batch_loss_d, te_mae_batch_loss_y, te_mse_batch_loss_d, te_mse_batch_loss_y, te_num_data, te_predicted, te_ground_truth, *t_loss = test(\n",
    "            model_name, batch_data, model, 'minmax', test_dataset.dataset.a_y, test_dataset.dataset.b_y,\n",
    "            test_dataset.dataset.a_d, test_dataset.dataset.b_d, use_treatment=not model_name=='iTransformer', MC_sample=1\n",
    "        )\n",
    "        \n",
    "        # 결과 저장\n",
    "        for i in range(te_num_data):\n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'data_index': itr * len(test_dataset) + i,\n",
    "                'predicted_y': te_predicted[i].item(),\n",
    "                'ground_truth_y': te_ground_truth[i].item(),\n",
    "                'mae_loss_y': te_mae_batch_loss_y[i].item(),\n",
    "                'mse_loss_y': te_mse_batch_loss_y[i].item(),\n",
    "                'mae_loss_d': te_mae_batch_loss_d[i].item(),\n",
    "                'mse_loss_d': te_mse_batch_loss_d[i].item(),\n",
    "                # 필요한 경우 추가 데이터 정보를 여기에 저장\n",
    "            })\n",
    "\n",
    "# 결과를 pandas DataFrame으로 변환\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# 모델별로 결과 분리\n",
    "df_cet = df[df['model'] == 'CETransformer']\n",
    "df_cevae = df[df['model'] == 'CEVAE']\n",
    "df_itransformer = df[df['model'] == 'iTransformer']\n",
    "\n",
    "# y와 d의 loss를 결합하여 전체 성능 평가\n",
    "def combined_loss(row):\n",
    "    return (row['mae_loss_y'] + row['mae_loss_d']) / 2  # 간단한 예시로 MAE의 평균을 사용\n",
    "\n",
    "df_cet['combined_loss'] = df_cet.apply(combined_loss, axis=1)\n",
    "df_cevae['combined_loss'] = df_cevae.apply(combined_loss, axis=1)\n",
    "df_itransformer['combined_loss'] = df_itransformer.apply(combined_loss, axis=1)\n",
    "\n",
    "# 성능 기준 설정 (예: 하위 25% 미만을 \"좋은\" 성능으로 간주)\n",
    "threshold_cet = np.percentile(df_cet['combined_loss'], 25)\n",
    "threshold_cevae = np.percentile(df_cevae['combined_loss'], 25)\n",
    "threshold_itransformer = np.percentile(df_itransformer['combined_loss'], 25)\n",
    "\n",
    "# CET와 CEVAE에서 좋은 성능을 보이면서 iTransformer에서 나쁜 성능을 보이는 케이스 찾기\n",
    "good_cet_cevae_bad_itransformer = df_cet[\n",
    "    (df_cet['combined_loss'] < threshold_cet) & \n",
    "    (df_cevae['combined_loss'] < threshold_cevae) & \n",
    "    (df_itransformer['combined_loss'] >= threshold_itransformer)\n",
    "]\n",
    "\n",
    "# CET와 iTransformer에서 좋은 성능을 보이면서 CEVAE에서 나쁜 성능을 보이는 케이스 찾기\n",
    "good_cet_itransformer_bad_cevae = df_cet[\n",
    "    (df_cet['combined_loss'] < threshold_cet) & \n",
    "    (df_itransformer['combined_loss'] < threshold_itransformer) & \n",
    "    (df_cevae['combined_loss'] >= threshold_cevae)\n",
    "]\n",
    "\n",
    "print(\"CET와 CEVAE에서 좋고 iTransformer에서 나쁜 케이스 수:\", len(good_cet_cevae_bad_itransformer))\n",
    "print(\"CET와 iTransformer에서 좋고 CEVAE에서 나쁜 케이스 수:\", len(good_cet_itransformer_bad_cevae))\n",
    "\n",
    "# 결과 저장\n",
    "good_cet_cevae_bad_itransformer.to_csv('good_cet_cevae_bad_itransformer.csv', index=False)\n",
    "good_cet_itransformer_bad_cevae.to_csv('good_cet_itransformer_bad_cevae.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
